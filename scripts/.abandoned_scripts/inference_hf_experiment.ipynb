{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bfcd6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ccebe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "DEVICE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d0b1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/shared/4/models/llama2/pytorch-versions/llama-2-7b-chat/\"\n",
    "data_dir = \"../data/mmlu/mmlu_mingqian.csv\"\n",
    "cache_dir= \"/shared/4/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e29bca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>subject</th>\n",
       "      <th>true_option</th>\n",
       "      <th>groundtruth</th>\n",
       "      <th>dataset</th>\n",
       "      <th>length</th>\n",
       "      <th>question_id</th>\n",
       "      <th>option1</th>\n",
       "      <th>option2</th>\n",
       "      <th>option3</th>\n",
       "      <th>option4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which of the following is the commonest cause ...</td>\n",
       "      <td>clinical_knowledge</td>\n",
       "      <td>1</td>\n",
       "      <td>Alzheimer's disease.</td>\n",
       "      <td>clinical_knowledge</td>\n",
       "      <td>13</td>\n",
       "      <td>486</td>\n",
       "      <td>Alzheimer's disease</td>\n",
       "      <td>Cerebrovascular (stroke) disease</td>\n",
       "      <td>Lewy body dementia</td>\n",
       "      <td>HIV infection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which of the following is true in diplopia?</td>\n",
       "      <td>clinical_knowledge</td>\n",
       "      <td>2</td>\n",
       "      <td>The outer image is always the false image</td>\n",
       "      <td>clinical_knowledge</td>\n",
       "      <td>8</td>\n",
       "      <td>421</td>\n",
       "      <td>Diplopia can never occur if one eye is covered</td>\n",
       "      <td>The outer image is always the false image</td>\n",
       "      <td>A fourth nerve palsy occurs when the patient l...</td>\n",
       "      <td>A sixth nerve palsy causes a divergent squint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fatty acids are transported into the mitochond...</td>\n",
       "      <td>clinical_knowledge</td>\n",
       "      <td>4</td>\n",
       "      <td>carnitine.</td>\n",
       "      <td>clinical_knowledge</td>\n",
       "      <td>9</td>\n",
       "      <td>404</td>\n",
       "      <td>thiokinase</td>\n",
       "      <td>coenzyme A (CoA)</td>\n",
       "      <td>acetyl-CoA</td>\n",
       "      <td>carnitine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which of the answers below best indicates the ...</td>\n",
       "      <td>clinical_knowledge</td>\n",
       "      <td>3</td>\n",
       "      <td>Blood type B (rhesus negative) and blood type ...</td>\n",
       "      <td>clinical_knowledge</td>\n",
       "      <td>25</td>\n",
       "      <td>446</td>\n",
       "      <td>Blood type AB (rhesus negative), blood type B,...</td>\n",
       "      <td>Blood type B (rhesus positive) and blood type ...</td>\n",
       "      <td>Blood type B (rhesus negative) and blood type ...</td>\n",
       "      <td>Blood type B (rhesus negative) only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one of the following represents a IIIrd ...</td>\n",
       "      <td>clinical_knowledge</td>\n",
       "      <td>3</td>\n",
       "      <td>Unilateral fixed dilated pupil.</td>\n",
       "      <td>clinical_knowledge</td>\n",
       "      <td>16</td>\n",
       "      <td>437</td>\n",
       "      <td>Unilateral constricted pupil</td>\n",
       "      <td>Bilateral constricted pupils</td>\n",
       "      <td>Unilateral fixed dilated pupil</td>\n",
       "      <td>Oval shaped pupils</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452</th>\n",
       "      <td>Which of the following is NOT a good predictor...</td>\n",
       "      <td>us_foreign_policy</td>\n",
       "      <td>4</td>\n",
       "      <td>Regime type</td>\n",
       "      <td>us_foreign_policy</td>\n",
       "      <td>16</td>\n",
       "      <td>1843</td>\n",
       "      <td>Receipt of sensitive nuclear assistance</td>\n",
       "      <td>Wealth/GDP</td>\n",
       "      <td>Rivalry with a nuclear state</td>\n",
       "      <td>Regime type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>What was 'democratic enlargement'?</td>\n",
       "      <td>us_foreign_policy</td>\n",
       "      <td>4</td>\n",
       "      <td>Both b and c</td>\n",
       "      <td>us_foreign_policy</td>\n",
       "      <td>4</td>\n",
       "      <td>1827</td>\n",
       "      <td>A proposal for reform of the US system of gove...</td>\n",
       "      <td>A proposal for the extension of democratic rul...</td>\n",
       "      <td>A proposal for the extension of free markets</td>\n",
       "      <td>Both b and c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2454</th>\n",
       "      <td>What is meant by the phrase 'empire by invitat...</td>\n",
       "      <td>us_foreign_policy</td>\n",
       "      <td>1</td>\n",
       "      <td>Voluntary reliance on an external power for se...</td>\n",
       "      <td>us_foreign_policy</td>\n",
       "      <td>9</td>\n",
       "      <td>1869</td>\n",
       "      <td>Voluntary reliance on an external power for se...</td>\n",
       "      <td>Willful openness to colonization</td>\n",
       "      <td>Cultural imperialism</td>\n",
       "      <td>Open advocacy of imperialism for economic gain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>In what way did the George W Bush administrati...</td>\n",
       "      <td>us_foreign_policy</td>\n",
       "      <td>1</td>\n",
       "      <td>It criticized international organizations, rat...</td>\n",
       "      <td>us_foreign_policy</td>\n",
       "      <td>16</td>\n",
       "      <td>1874</td>\n",
       "      <td>It criticized international organizations, rat...</td>\n",
       "      <td>It expanded NATO to include former Soviet states</td>\n",
       "      <td>It focused on a more personal style of leadership</td>\n",
       "      <td>It increased international support for the Uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>Peace, commerce, and honest friendship with al...</td>\n",
       "      <td>us_foreign_policy</td>\n",
       "      <td>4</td>\n",
       "      <td>Thomas Jefferson</td>\n",
       "      <td>us_foreign_policy</td>\n",
       "      <td>15</td>\n",
       "      <td>1861</td>\n",
       "      <td>James Madison</td>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>Woodrow Wilson</td>\n",
       "      <td>Thomas Jefferson</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2457 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question             subject  \\\n",
       "0     Which of the following is the commonest cause ...  clinical_knowledge   \n",
       "1           Which of the following is true in diplopia?  clinical_knowledge   \n",
       "2     Fatty acids are transported into the mitochond...  clinical_knowledge   \n",
       "3     Which of the answers below best indicates the ...  clinical_knowledge   \n",
       "4     Which one of the following represents a IIIrd ...  clinical_knowledge   \n",
       "...                                                 ...                 ...   \n",
       "2452  Which of the following is NOT a good predictor...   us_foreign_policy   \n",
       "2453                 What was 'democratic enlargement'?   us_foreign_policy   \n",
       "2454  What is meant by the phrase 'empire by invitat...   us_foreign_policy   \n",
       "2455  In what way did the George W Bush administrati...   us_foreign_policy   \n",
       "2456  Peace, commerce, and honest friendship with al...   us_foreign_policy   \n",
       "\n",
       "      true_option                                        groundtruth  \\\n",
       "0               1                               Alzheimer's disease.   \n",
       "1               2          The outer image is always the false image   \n",
       "2               4                                         carnitine.   \n",
       "3               3  Blood type B (rhesus negative) and blood type ...   \n",
       "4               3                    Unilateral fixed dilated pupil.   \n",
       "...           ...                                                ...   \n",
       "2452            4                                        Regime type   \n",
       "2453            4                                       Both b and c   \n",
       "2454            1  Voluntary reliance on an external power for se...   \n",
       "2455            1  It criticized international organizations, rat...   \n",
       "2456            4                                   Thomas Jefferson   \n",
       "\n",
       "                 dataset  length  question_id  \\\n",
       "0     clinical_knowledge      13          486   \n",
       "1     clinical_knowledge       8          421   \n",
       "2     clinical_knowledge       9          404   \n",
       "3     clinical_knowledge      25          446   \n",
       "4     clinical_knowledge      16          437   \n",
       "...                  ...     ...          ...   \n",
       "2452   us_foreign_policy      16         1843   \n",
       "2453   us_foreign_policy       4         1827   \n",
       "2454   us_foreign_policy       9         1869   \n",
       "2455   us_foreign_policy      16         1874   \n",
       "2456   us_foreign_policy      15         1861   \n",
       "\n",
       "                                                option1  \\\n",
       "0                                   Alzheimer's disease   \n",
       "1        Diplopia can never occur if one eye is covered   \n",
       "2                                            thiokinase   \n",
       "3     Blood type AB (rhesus negative), blood type B,...   \n",
       "4                          Unilateral constricted pupil   \n",
       "...                                                 ...   \n",
       "2452            Receipt of sensitive nuclear assistance   \n",
       "2453  A proposal for reform of the US system of gove...   \n",
       "2454  Voluntary reliance on an external power for se...   \n",
       "2455  It criticized international organizations, rat...   \n",
       "2456                                      James Madison   \n",
       "\n",
       "                                                option2  \\\n",
       "0                      Cerebrovascular (stroke) disease   \n",
       "1             The outer image is always the false image   \n",
       "2                                      coenzyme A (CoA)   \n",
       "3     Blood type B (rhesus positive) and blood type ...   \n",
       "4                          Bilateral constricted pupils   \n",
       "...                                                 ...   \n",
       "2452                                         Wealth/GDP   \n",
       "2453  A proposal for the extension of democratic rul...   \n",
       "2454                   Willful openness to colonization   \n",
       "2455   It expanded NATO to include former Soviet states   \n",
       "2456                                    Abraham Lincoln   \n",
       "\n",
       "                                                option3  \\\n",
       "0                                    Lewy body dementia   \n",
       "1     A fourth nerve palsy occurs when the patient l...   \n",
       "2                                            acetyl-CoA   \n",
       "3     Blood type B (rhesus negative) and blood type ...   \n",
       "4                        Unilateral fixed dilated pupil   \n",
       "...                                                 ...   \n",
       "2452                       Rivalry with a nuclear state   \n",
       "2453       A proposal for the extension of free markets   \n",
       "2454                               Cultural imperialism   \n",
       "2455  It focused on a more personal style of leadership   \n",
       "2456                                     Woodrow Wilson   \n",
       "\n",
       "                                                option4  \n",
       "0                                         HIV infection  \n",
       "1         A sixth nerve palsy causes a divergent squint  \n",
       "2                                             carnitine  \n",
       "3                   Blood type B (rhesus negative) only  \n",
       "4                                    Oval shaped pupils  \n",
       "...                                                 ...  \n",
       "2452                                        Regime type  \n",
       "2453                                       Both b and c  \n",
       "2454     Open advocacy of imperialism for economic gain  \n",
       "2455  It increased international support for the Uni...  \n",
       "2456                                   Thomas Jefferson  \n",
       "\n",
       "[2457 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv(data_dir)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10c3e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{{You will be presented with a role-playing context followed by a multiple-choice question. {role_context} Select only the option number that corresponds to the correct answer for the following question.}}\\n\\n### Input:\\n{{{{{question}}} Provide the number of the correct option without explaining your reasoning.}} \\n\\n### Response:'''\n",
    "flan_template = '''{role_context} {question} Please select the correct answer number:'''\n",
    "role_context = \"You are a helpful assistant.\"\n",
    "\n",
    "answer_prompts = []\n",
    "for idx, item in data_df.iterrows():\n",
    "    question_text = item['question']\n",
    "    option1 = item[\"option1\"]\n",
    "    option2 = item[\"option2\"]\n",
    "    option3 = item[\"option3\"]\n",
    "    option4 = item[\"option4\"]\n",
    "\n",
    "    choices_text = f'Options: 1. {option1}, 2. {option2}, 3. {option3}, 4. {option4}.'\n",
    "    question_text = f\"{question_text} {choices_text}\"\n",
    "    full_prompt = template.format(role_context=role_context, question=question_text)\n",
    "    answer_prompts.append(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eb25c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(model_dir, \n",
    "                                           cache_dir=cache_dir,\n",
    "                                           padding_side='left',\n",
    "                                           )\n",
    "\n",
    "if not tokenizer.pad_token:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e30793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b134f0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02739599af374b10bb4a848b121ab7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 23.68 GiB of which 60.12 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 23.42 GiB is allocated by PyTorch, and 1.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m LlamaForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_dir, \n\u001b[1;32m      3\u001b[0m                                          cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m      4\u001b[0m                                          \u001b[38;5;66;03m#device_map=\"auto\",\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#                                         quantization_config=quantization_config,\u001b[39;00m\n\u001b[1;32m      6\u001b[0m                                          \u001b[38;5;66;03m#load_in_8bit=True\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m                                         )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/transformers/modeling_utils.py:2556\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2554\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2555\u001b[0m         )\n\u001b[0;32m-> 2556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 23.68 GiB of which 60.12 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 23.42 GiB is allocated by PyTorch, and 1.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = LlamaForCausalLM.from_pretrained(model_dir, \n",
    "                                         cache_dir=cache_dir,\n",
    "                                         #device_map=\"auto\",\n",
    "#                                         quantization_config=quantization_config,\n",
    "                                         #load_in_8bit=True\n",
    "                                        ).to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = tokenizer(\"hi man\", return_tensors='pt')\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f20f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "model.generate(**q.to(\"0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "474264aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "ques_batch = answer_prompts[0:(0+BATCH_SIZE)]\n",
    "ques_batch_tokenized = tokenizer(ques_batch, return_tensors='pt', truncation=True, max_length=512, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ad1913a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,     2,     2,  ..., 29937, 13291, 29901],\n",
       "        [    2,     2,     2,  ..., 29937, 13291, 29901],\n",
       "        [    2,     2,     2,  ..., 29937, 13291, 29901],\n",
       "        ...,\n",
       "        [    2,     2,     2,  ..., 29937, 13291, 29901],\n",
       "        [    2,     2,     2,  ..., 29937, 13291, 29901],\n",
       "        [    2,     2,     1,  ..., 29937, 13291, 29901]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques_batch_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d924121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "answ_generated = model.generate(**ques_batch_tokenized, max_new_tokens=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b32055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/anaconda/lib/python3.11/site-packages/transformers/generation/utils.py:1477: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "answ_generated = model.generate(**ques_batch_tokenized, max_length=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de9c027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/2 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/anaconda/lib/python3.11/site-packages/transformers/generation/utils.py:1477: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for idx in tqdm(range(0, 12, BATCH_SIZE)):\n",
    "    ques_batch = answer_prompts[idx:(idx+BATCH_SIZE)]\n",
    "    ques_batch_tokenized = tokenizer(ques_batch, return_tensors='pt', truncation=True, max_length=512, padding=True)\n",
    "    answ_generated = model.generate(**ques_batch_tokenized, max_new_tokens=30)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68eeaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.from_list(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cef5920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
